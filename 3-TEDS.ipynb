{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0843a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from Utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb52a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "File_C, Target_C, FID_C  = 'Data/carevue.csv',    0, [211, 646, 618, 51, 8368]                  # File Path, Prediction ID, Feature IDs [for carevue]\n",
    "File_M, Target_M, FID_M  = 'Data/metavision.csv', 0, [220045, 220210, 220277, 220181, 220179]   # File Path, Prediction ID, Feature IDs [for metavision]\n",
    "\n",
    "W     = 3             # Window Size\n",
    "UP    = 50            # Update Period\n",
    "EPOC  = 50            # Number of epoch\n",
    "alpha = 0.2           # Weight of heterogeneous transfer\n",
    "\n",
    "NF_C  = len(FID_C)-1  # Number of features [for carevue]\n",
    "NF_M  = len(FID_M)-1  # Number of features [for metavision]\n",
    "loss_mae = torch.nn.L1Loss()\n",
    "loss_mse = torch.nn.MSELoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XSC_Tra,XDC_Tra,YC_Tra,BC_Tra, XSC_Val,XDC_Val,YC_Val,BC_Val, XSC_Tes,XDC_Tes,YC_Tes,BC_Tes = load_data_SD(File_C, [0.6, 0.2, 0.2], FID_C, Target_C, W, device)\n",
    "# XSM_Tra,XDM_Tra,YM_Tra,BM_Tra, XSM_Val,XDM_Val,YM_Val,BM_Val, XSM_Tes,XDM_Tes,YM_Tes,BM_Tes = load_data_SD(File_M, [0.6, 0.2, 0.2], FID_M, Target_M, W, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, n_time, n_embv):\n",
    "        super(AE, self).__init__()\n",
    "        ed0, ed1, ed2 = 32, 256, 24\n",
    "        self.encoder  = torch.nn.Sequential(torch.nn.Linear(n_time, ed0), torch.nn.LeakyReLU(), torch.nn.Linear(ed0, ed1), torch.nn.LeakyReLU(),torch.nn.Linear(ed1, ed2), torch.nn.LeakyReLU(), torch.nn.Linear(ed2, n_embv))\n",
    "        self.decoder  = torch.nn.Sequential(torch.nn.Linear(n_embv, ed2), torch.nn.LeakyReLU(), torch.nn.Linear(ed2, ed1), torch.nn.LeakyReLU(),torch.nn.Linear(ed1, ed0), torch.nn.LeakyReLU(), torch.nn.Linear(ed0, n_time))\n",
    "    def forward(self, x):\n",
    "        embv = self.encoder(x)\n",
    "        recv = self.decoder(embv)\n",
    "        return [embv, recv]\n",
    "\n",
    "class PL(torch.nn.Module):\n",
    "    def __init__(self, n_fea, n_embv):\n",
    "        super(PL, self).__init__()\n",
    "        ed0, ed1, ed2 = 32, 256, 16\n",
    "        self.Linear    = torch.nn.Sequential(torch.nn.Linear(n_fea*n_embv*2, ed0), torch.nn.LeakyReLU(), torch.nn.Linear(ed0, ed1), torch.nn.LeakyReLU(),torch.nn.Linear(ed1, ed2), torch.nn.LeakyReLU(), torch.nn.Linear(ed2, 1))\n",
    "    def forward(self, x):\n",
    "        return self.Linear(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPara  = 0\n",
    "Net_AE, Net_PL = AE(n_time=W, n_embv=1), PL(n_fea =NF_C, n_embv=1)\n",
    "paras  = filter(lambda p: p.requires_grad, Net_AE.parameters())\n",
    "NPara += sum([np.prod(p.size()) for p in paras])*NF_C\n",
    "paras  = filter(lambda p: p.requires_grad, Net_PL.parameters())\n",
    "NPara += sum([np.prod(p.size()) for p in paras])\n",
    "print('Parameters:', NPara)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581b8e6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad07494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "BL_C, PATH_C = np.inf, 'Model/3-TEDS/Model-care-'+str(Target_C)+'-'+str(Target_M)+'_'+str(IterID)  # Best Loss, Model Saved Path\n",
    "BL_M, PATH_M = np.inf, 'Model/3-TEDS/Model-meta-'+str(Target_C)+'-'+str(Target_M)+'_'+str(IterID)\n",
    "Tnet_AEC, Tnet_PLC = [AE(n_time=W,  n_embv=1) for F in range(NF_C)], PL(n_fea =NF_C, n_embv=1)     # Temporary Models\n",
    "Tnet_AEM, Tnet_PLM = [AE(n_time=W,  n_embv=1) for F in range(NF_M)], PL(n_fea =NF_M, n_embv=1)\n",
    "net_AEC,   net_PLC = [AE(n_time=W,  n_embv=1) for F in range(NF_C)], PL(n_fea =NF_C, n_embv=1)     # Training Model\n",
    "net_AEM,   net_PLM = [AE(n_time=W,  n_embv=1) for F in range(NF_M)], PL(n_fea =NF_M, n_embv=1)\n",
    "for F in range(NF_C): net_AEC[F].to(device)\n",
    "for F in range(NF_M): net_AEM[F].to(device)\n",
    "net_PLC.to(device)\n",
    "net_PLM.to(device)\n",
    "\n",
    "for epoc in range(EPOC):\n",
    "    for d in range(2):     # 0=Train, 1=Valid\n",
    "        if d==0:     # Train\n",
    "            XDC, XSC, YC, BC = XDC_Tra, XSC_Tra, YC_Tra, BC_Tra\n",
    "            XDM, XSM, YM, BM = XDM_Tra, XSM_Tra, YM_Tra, BM_Tra\n",
    "        if d==1:     # Valid\n",
    "            XDC, XSC, YC, BC = XDC_Val, XSC_Val, YC_Val, BC_Val\n",
    "            XDM, XSM, YM, BM = XDM_Val, XSM_Val, YM_Val, BM_Val\n",
    "            Tnet_AEC, Tnet_PLC = copy.deepcopy(net_AEC), copy.deepcopy(net_PLC)     # Save the origin model\n",
    "            Tnet_AEM, Tnet_PLM = copy.deepcopy(net_AEM), copy.deepcopy(net_PLM)\n",
    "        optimizer_HC, optimizer_RC = [torch.optim.Adam(net_AEC[F].parameters(), lr=0.0025) for F in range(NF_C)], torch.optim.Adam(net_PLC.parameters(), lr=0.01)\n",
    "        optimizer_HM, optimizer_RM = [torch.optim.Adam(net_AEM[F].parameters(), lr=0.0025) for F in range(NF_M)], torch.optim.Adam(net_PLM.parameters(), lr=0.01)\n",
    "        Pred_C, SL_C = torch.FloatTensor([]).to(device), []\n",
    "        Pred_M, SL_M = torch.FloatTensor([]).to(device), []\n",
    "        NRC   = [BC[i+1]-BC[i] for i in range(len(BC)-1)]                           # Number of records of each patient\n",
    "        NRM   = [BM[i+1]-BM[i] for i in range(len(BM)-1)]\n",
    "\n",
    "        for b in range( max([max(NRC), max(NRM)])//UP ):\n",
    "            if d==0 and b%5==4: print(epoc, 'Tra', b+1, '/', max([max(NRC), max(NRM)])//UP, end='     \\r')\n",
    "            if d==1 and b%5==4: print(epoc, 'Val', b+1, '/', max([max(NRC), max(NRM)])//UP, end='     \\r')\n",
    "            sl_C, sl_M = [], []\n",
    "            for p in range(len(BC)-1):\n",
    "                if NRC[p]>=UP*(b+1): sl_C += [BC[p]+UP*b+i for i in range(UP)]\n",
    "            for p in range(len(BM)-1):\n",
    "                if NRM[p]>=UP*(b+1): sl_M += [BM[p]+UP*b+i for i in range(UP)]        \n",
    "\n",
    "            # Heterogeneous Transfer (Training Phase Only)\n",
    "            if d==0:\n",
    "                # Find the similar AE for Care from Meta\n",
    "                Tnet_AEC, Tnet_AEM, SAE_C = copy.deepcopy(net_AEC), copy.deepcopy(net_AEM), []\n",
    "                for FC in range(NF_C):\n",
    "                    LL = []   # Loss list\n",
    "                    for FM in range(NF_M):\n",
    "                        ev, rv = net_AEM[FM](XDC[sl_C,FC,:])\n",
    "                        LL.append(float(loss_mae(rv.detach(), XDC[sl_C,FC,:])))\n",
    "                    SAE_C.append(np.argmin(LL))\n",
    "                # Weighted\n",
    "                for F in range(NF_C): \n",
    "                    sdC = Tnet_AEC[F].state_dict()\n",
    "                    sdM = Tnet_AEM[SAE_C[F]].state_dict()\n",
    "                    for key in sdC: sdC[key] = (1-alpha)*sdC[key] + alpha*sdM[key]\n",
    "                    net_AEC[F].load_state_dict(sdC)\n",
    "            if d==0:   \n",
    "                Tnet_AEC, Tnet_AEM, SAE_M = copy.deepcopy(net_AEC), copy.deepcopy(net_AEM), []\n",
    "                for FM in range(NF_M):\n",
    "                    LL = []   # Loss list\n",
    "                    for FC in range(NF_C):\n",
    "                        ev, rv = net_AEC[FC](XDM[sl_M,FM,:])\n",
    "                        LL.append(float(loss_mae(rv.detach(), XDM[sl_M,FM,:])))\n",
    "                    SAE_M.append(np.argmin(LL))\n",
    "                for F in range(NF_M):\n",
    "                    sdC = Tnet_AEC[SAE_M[F]].state_dict()\n",
    "                    sdM = Tnet_AEM[F].state_dict()\n",
    "                    for key in sdM: sdM[key] = (1-alpha)*sdM[key] + alpha*sdC[key]\n",
    "                    net_AEM[F].load_state_dict(sdM)\n",
    "                    \n",
    "            # Care            \n",
    "            EV, RV = net_AEC[0]( XDC[sl_C,0,:] )\n",
    "            RV     = RV[:,None,:]\n",
    "            for FC in range(1,NF_C):     # Embed Dense Feature\n",
    "                ev, rv = net_AEC[FC]( XDC[sl_C,FC,:] )\n",
    "                EV = torch.cat((EV,ev), dim=1)\n",
    "                RV = torch.cat((RV,rv[:,None,:]), dim=1)\n",
    "            for FC in range(NF_C):       # Embed Spare Feature\n",
    "                ev, rv = net_AEC[F]( XSC[sl_C,FC,:] )\n",
    "                EV = torch.cat((EV,ev), dim=1)\n",
    "            pred = net_PLC(EV)\n",
    "            for FC in range(NF_C):       # Update AEs\n",
    "                loss = loss_mae(RV[:,FC], XDC[sl_C,FC,:])\n",
    "                loss.backward(retain_graph=True)\n",
    "            loss = loss_mse(torch.reshape(pred,(-1,)), YC[sl_C])\n",
    "            loss.backward()\n",
    "            for FC in range(NF_C): optimizer_HC[FC].step()\n",
    "            for FC in range(NF_C): optimizer_HC[FC].zero_grad()\n",
    "            optimizer_RC.step()\n",
    "            optimizer_RC.zero_grad()\n",
    "            Pred_C = torch.cat((Pred_C,torch.reshape(pred,(-1,)).detach()))\n",
    "            SL_C  += sl_C\n",
    "            \n",
    "            # Meta\n",
    "            EV, RV = net_AEM[0]( XDM[sl_M,0,:] )\n",
    "            RV     = RV[:,None,:]\n",
    "            for FM in range(1,NF_M):     # Embed Dense Feature\n",
    "                ev, rv = net_AEM[FM]( XDM[sl_M,FM,:] )\n",
    "                EV = torch.cat((EV,ev), dim=1)\n",
    "                RV = torch.cat((RV,rv[:,None,:]), dim=1)\n",
    "            for FM in range(NF_M):       # Embed Spare Feature\n",
    "                ev, rv = net_AEM[FM]( XSM[sl_M,FM,:] )\n",
    "                EV = torch.cat((EV,ev), dim=1)\n",
    "            pred = net_PLM(EV)\n",
    "            for FM in range(NF_M):       # Update AEs\n",
    "                loss = loss_mae(RV[:,FM], XDM[sl_M,FM,:])\n",
    "                loss.backward(retain_graph=True)\n",
    "            loss = loss_mse(torch.reshape(pred,(-1,)), YM[sl_M])\n",
    "            loss.backward()\n",
    "            for FM in range(NF_M): optimizer_HM[FM].step()\n",
    "            for FM in range(NF_M): optimizer_HM[FM].zero_grad()\n",
    "            optimizer_RM.step()\n",
    "            optimizer_RM.zero_grad()\n",
    "            Pred_M = torch.cat((Pred_M,torch.reshape(pred,(-1,)).detach()))\n",
    "            SL_M  += sl_M\n",
    "\n",
    "        if d==1:     # Save Best (Validation Phase Only)\n",
    "            loss_C, loss_M = loss_mse(Pred_C, YC[SL_C]), loss_mse(Pred_M, YM[SL_M])\n",
    "            if loss_C<BL_C:\n",
    "                BL_C = loss_C\n",
    "                torch.save([Tnet_PLC.state_dict()]+[Tnet_AEC[F].state_dict() for F in range(NF_C)], PATH_C+'.pt') \n",
    "                print('Save Best Care at', epoc, 'with loss of',  round(float(loss_C),3))\n",
    "            if loss_M<BL_M:\n",
    "                BL_M = loss_M\n",
    "                torch.save([Tnet_PLM.state_dict()]+[Tnet_AEM[F].state_dict() for F in range(NF_M)], PATH_M+'.pt') \n",
    "                print('Save Best Meta at', epoc, 'with loss of',  round(float(loss_M),3))\n",
    "            if epoc%10==9: print(epoc+1, '/', EPOC, ' ', round(float(loss_C),3), round(float(loss_M),3), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455faae",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "TVTLoss_C, TVTLoss_M = np.zeros(3), np.zeros(3)     # Train/Valid/Test Loss\n",
    "XSC_Tra,XDC_Tra,YC_Tra,BC_Tra, XSC_Val,XDC_Val,YC_Val,BC_Val, XSC_Tes,XDC_Tes,YC_Tes,BC_Tes = load_data_SD(File_C, [0.6, 0.2, 0.2], FID_C, Target_C, W, device)\n",
    "XSM_Tra,XDM_Tra,YM_Tra,BM_Tra, XSM_Val,XDM_Val,YM_Val,BM_Val, XSM_Tes,XDM_Tes,YM_Tes,BM_Tes = load_data_SD(File_M, [0.6, 0.2, 0.2], FID_M, Target_M, W, device)\n",
    "Tnet_AEC, Tnet_PLC = [AE(n_time=W,  n_embv=1) for F in range(NF_C)], PL(n_fea =NF_C, n_embv=1)     # Temporary Models (for Care)\n",
    "Tnet_AEM, Tnet_PLM = [AE(n_time=W,  n_embv=1) for F in range(NF_M)], PL(n_fea =NF_M, n_embv=1)     # Temporary Models (for Meta)\n",
    "net_AEM,   net_PLM = [AE(n_time=W,  n_embv=1) for F in range(NF_M)], PL(n_fea =NF_M, n_embv=1)     # Saved Model\n",
    "net_AEC,   net_PLC = [AE(n_time=W,  n_embv=1) for F in range(NF_C)], PL(n_fea =NF_C, n_embv=1)     # Training Model\n",
    "for F in range(NF_C): net_AEC[F].to(device)\n",
    "for F in range(NF_M): net_AEM[F].to(device)\n",
    "net_PLC.to(device)\n",
    "net_PLM.to(device)\n",
    "\n",
    "PATH_C = 'Model/3-TEDS/Model-care-'+str(Target_C)+'-'+str(Target_M)+'_'+str(IterID)     # Best Loss, Model Saved Path\n",
    "PATH_M = 'Model/3-TEDS/Model-meta-'+str(Target_C)+'-'+str(Target_M)+'_'+str(IterID)\n",
    "# Load Saved Weightes\n",
    "Weights_C = torch.load(PATH_C+'.pt')\n",
    "Weights_M = torch.load(PATH_M+'.pt')  \n",
    "Tnet_PLC.load_state_dict(Weights_C[0])\n",
    "Tnet_PLM.load_state_dict(Weights_M[0])\n",
    "for F in range(NF_C): Tnet_AEC[F].load_state_dict(Weights_C[1+F])\n",
    "for F in range(NF_M): Tnet_AEM[F].load_state_dict(Weights_M[1+F])\n",
    "\n",
    "for d in range(3):     # 0=Train, 1=Valid, 2=Test\n",
    "    # Load Saved Weights\n",
    "    net_PLC, net_PLM = copy.deepcopy(Tnet_PLC), copy.deepcopy(Tnet_PLM)\n",
    "    for F in range(NF_C): net_AEC[F] = copy.deepcopy(Tnet_AEC[F])\n",
    "    for F in range(NF_M): net_AEM[F] = copy.deepcopy(Tnet_AEM[F])\n",
    "    if d==0: XDC,XSC,YC,BC, XDM,XSM,YM,BM = XDC_Tra,XSC_Tra,YC_Tra,BC_Tra, XDM_Tra,XSM_Tra,YM_Tra,BM_Tra     # Training\n",
    "    if d==1: XDC,XSC,YC,BC, XDM,XSM,YM,BM = XDC_Val,XSC_Val,YC_Val,BC_Val, XDM_Val,XSM_Val,YM_Val,BM_Val     # Validation\n",
    "    if d==2: XDC,XSC,YC,BC, XDM,XSM,YM,BM = XDC_Tes,XSC_Tes,YC_Tes,BC_Tes, XDM_Tes,XSM_Tes,YM_Tes,BM_Tes     # Testing\n",
    "    optimizer_HC, optimizer_RC = [torch.optim.Adam(net_AEC[F].parameters(), lr=0.0025) for F in range(NF_C)], torch.optim.Adam(net_PLC.parameters(), lr=0.01)\n",
    "    optimizer_HM, optimizer_RM = [torch.optim.Adam(net_AEM[F].parameters(), lr=0.0025) for F in range(NF_M)], torch.optim.Adam(net_PLM.parameters(), lr=0.01)\n",
    "    Pred_C, SL_C = torch.FloatTensor([]).to(device), []\n",
    "    Pred_M, SL_M = torch.FloatTensor([]).to(device), []\n",
    "    NRC, NRM     = [BC[i+1]-BC[i] for i in range(len(BC)-1)], [BM[i+1]-BM[i] for i in range(len(BM)-1)]      # Number of records of each patient, Flag for update\n",
    "\n",
    "    for b in range( max([max(NRC), max(NRM)])//UP ):\n",
    "        if d==0 and b%5==4: print('Tra', b+1, '/', max([max(NRC), max(NRM)])//UP, end='     \\r')\n",
    "        if d==1 and b%5==4: print('Val', b+1, '/', max([max(NRC), max(NRM)])//UP, end='     \\r')\n",
    "        if d==2 and b%5==4: print('Tes', b+1, '/', max([max(NRC), max(NRM)])//UP, end='     \\r')\n",
    "        sl_C, sl_M = [], []\n",
    "        for p in range(len(BC)-1):\n",
    "            if NRC[p]>=UP*(b+1): sl_C += [BC[p]+UP*b+i for i in range(UP)]\n",
    "        for p in range(len(BM)-1):\n",
    "            if NRM[p]>=UP*(b+1): sl_M += [BM[p]+UP*b+i for i in range(UP)]         \n",
    "        # Care            \n",
    "        EV, RV = net_AEC[0]( XDC[sl_C,0,:] )\n",
    "        RV     = RV[:,None,:]\n",
    "        for FC in range(1,NF_C):     # Embed Dense Feature\n",
    "            ev, rv = net_AEC[FC]( XDC[sl_C,FC,:] )\n",
    "            EV = torch.cat((EV,ev), dim=1)\n",
    "            RV = torch.cat((RV,rv[:,None,:]), dim=1)\n",
    "        for FC in range(NF_C):       # Embed Spare Feature\n",
    "            ev, rv = net_AEC[F]( XSC[sl_C,FC,:] )\n",
    "            EV = torch.cat((EV,ev), dim=1)\n",
    "        pred = net_PLC(EV)\n",
    "        for FC in range(NF_C):       # Update AEs\n",
    "            loss = loss_mae(RV[:,FC], XDC[sl_C,FC,:])\n",
    "            loss.backward(retain_graph=True)\n",
    "        loss = loss_mse(torch.reshape(pred,(-1,)), YC[sl_C])\n",
    "        loss.backward()\n",
    "        for FC in range(NF_C): optimizer_HC[FC].step()\n",
    "        for FC in range(NF_C): optimizer_HC[FC].zero_grad()\n",
    "        optimizer_RC.step()\n",
    "        optimizer_RC.zero_grad()\n",
    "        Pred_C = torch.cat((Pred_C,torch.reshape(pred,(-1,)).detach()))\n",
    "        SL_C  += sl_C\n",
    "        \n",
    "        # Meta\n",
    "        EV, RV = net_AEM[0]( XDM[sl_M,0,:] )\n",
    "        RV     = RV[:,None,:]\n",
    "        for FM in range(1,NF_M):     # Embed Dense Feature\n",
    "            ev, rv = net_AEM[FM]( XDM[sl_M,FM,:] )\n",
    "            EV = torch.cat((EV,ev), dim=1)\n",
    "            RV = torch.cat((RV,rv[:,None,:]), dim=1)\n",
    "        for FM in range(NF_M):       # Embed Spare Feature\n",
    "            ev, rv = net_AEM[FM]( XSM[sl_M,FM,:] )\n",
    "            EV = torch.cat((EV,ev), dim=1)\n",
    "        pred = net_PLM(EV)\n",
    "        for FM in range(NF_M):       # Update AEs\n",
    "            loss = loss_mae(RV[:,FM], XDM[sl_M,FM,:])\n",
    "            loss.backward(retain_graph=True)\n",
    "        loss = loss_mse(torch.reshape(pred,(-1,)), YM[sl_M])\n",
    "        loss.backward()\n",
    "        for FM in range(NF_M): optimizer_HM[FM].step()\n",
    "        for FM in range(NF_M): optimizer_HM[FM].zero_grad()\n",
    "        optimizer_RM.step()\n",
    "        optimizer_RM.zero_grad()\n",
    "        Pred_M = torch.cat((Pred_M,torch.reshape(pred,(-1,)).detach()))\n",
    "        SL_M  += sl_M\n",
    "\n",
    "    loss_C, loss_M = loss_mse(Pred_C, YC[SL_C]), loss_mse(Pred_M, YM[SL_M])\n",
    "    TVTLoss_C[d] = float(loss_C)\n",
    "    TVTLoss_M[d] = float(loss_M)\n",
    "print('Care')\n",
    "print('Training   Loss', round(float(TVTLoss_C[0]),3))\n",
    "print('Validation Loss', round(float(TVTLoss_C[1]),3))\n",
    "print('Testing    Loss', round(float(TVTLoss_C[2]),3))\n",
    "print('Meta')\n",
    "print('Training   Loss', round(float(TVTLoss_M[0]),3))\n",
    "print('Validation Loss', round(float(TVTLoss_M[1]),3))\n",
    "print('Testing    Loss', round(float(TVTLoss_M[2]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
